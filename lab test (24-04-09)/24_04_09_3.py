# -*- coding: utf-8 -*-
"""24-04-09-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1milDD4T5nuARraSBjQZyBhTB0P8fWgRQ
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OrdinalEncoder

df= pd.read_csv('collected_data.csv')
df.head()

df.loc[df['Annual family income'] < 100000, 'Annual family income'] *= 100000

df.head()

cat_cols=[]
num_cols=[]
for it in df.columns:
    if(df[it].dtype=='object'):
        cat_cols.append(it)
    else:
        num_cols.append(it)

cat_cols

num_cols

max,mc=0,0
for cat in num_cols:
    print(f"{cat}: ", end='')
    mis=df[cat].isnull().sum()
    if mis>max:
        max=mis
        mc=cat
    print((mis*100)/len(df))
max=(max*100)/len(df)
print()
print(f"{mc} has the most missing data: {max}")

for cat in num_cols:
    df[cat].fillna(df[cat].median(),inplace=True)

for cat in cat_cols:
    df[cat].fillna(df[cat].mode()[0], inplace=True)

encoder = OrdinalEncoder()
encoder.fit(df[cat_cols])
df[cat_cols]=encoder.transform(df[cat_cols])
df

for cat in num_cols:
    IQR=df[cat].quantile(0.75)-df[cat].quantile(0.25)
    UB= df[cat].quantile(0.75)+ (1.5*IQR)
    LB= df[cat].quantile(0.25)-(1.5*IQR)
    s=0
    for row in df[cat]:
        if row>UB or row< LB:
            s+=1
    print(f"{cat}: {s}")

for cat in num_cols:
    IQR=df[cat].quantile(0.75)-df[cat].quantile(0.25)
    UB= df[cat].quantile(0.75)+ (1.5*IQR)
    LB= df[cat].quantile(0.25)-(1.5*IQR)

    df[cat] = np.where(df[cat] >UB, UB,df[cat])
    df[cat] = np.where(df[cat] <LB, LB, df[cat])

for cat in num_cols:
    IQR=df[cat].quantile(0.75)-df[cat].quantile(0.25)
    UB= df[cat].quantile(0.75)+ (1.5*IQR)
    LB= df[cat].quantile(0.25)-(1.5*IQR)
    s=0
    for row in df[cat]:
        if row>UB or row< LB:
            s+=1
    print(f"{cat}: {s}")

df.head()

print(df.corr())

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = df.corr()

plt.figure(figsize=(10, 8))

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")

plt.title('Correlation Matrix')
plt.show()

print(correlation_matrix)

correlation_matrix.shape

correlation_matrix.iloc[-1,:].sort_values(ascending=False)

"""### Factors affecting willingness to pay are:
Annual family income  
Timestamp  
Father's education level  
Area type  
No of sisters  
Position among siblings  
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, f1_score
from statsmodels.tools import add_constant
from statsmodels.discrete.discrete_model import Logit
from sklearn.metrics import accuracy_score


df['Willingness to pay (in INR)'] = df['Willingness to pay (in INR)'].apply(lambda x: 'High' if x >= 1000 else 'Low')
df.head()

from sklearn.preprocessing import OrdinalEncoder

encoder1 = OrdinalEncoder()

df_column_reshaped = df['Willingness to pay (in INR)'].values.reshape(-1, 1)

encoder1.fit(df_column_reshaped)

df['Willingness to pay (in INR)'] = encoder1.transform(df_column_reshaped)

X = df.drop([ 'Willingness to pay (in INR)'], axis=1)
y = df['Willingness to pay (in INR)']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

df.head()

svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)

svm_pred = svm_model.predict(X_test)
dt_pred = dt_model.predict(X_test)
logreg_pred = logreg_model.predict(X_test)

svm_conf_matrix = confusion_matrix(y_test, svm_pred)
dt_conf_matrix = confusion_matrix(y_test, dt_pred)
logreg_conf_matrix = confusion_matrix(y_test, logreg_pred)

svm_f1 = f1_score(y_test, svm_pred)
dt_f1 = f1_score(y_test, dt_pred)
logreg_f1 = f1_score(y_test, logreg_pred)

X_train_const = add_constant(X_train)
logit_model = Logit(y_train, X_train_const)
result = logit_model.fit()
logreg_aic = result.aic
logreg_bic = result.bic

print("Support Vector Machine:")
print("Confusion Matrix:")
print(svm_conf_matrix)
print("F1 Score:", svm_f1)

print("\nDecision Tree:")
print("Confusion Matrix:")
print(dt_conf_matrix)
print("F1 Score:", dt_f1)

print("\nLogistic Regression:")
print("Confusion Matrix:")
print(logreg_conf_matrix)
print("F1 Score:", logreg_f1)
print("AIC:", logreg_aic)
print("BIC:", logreg_bic)